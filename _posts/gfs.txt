---
title: 谷歌的马车们（一）：Google File System
date: 2016-08-13 00:06:14
tags: 分布式系统
---

google的几篇论文算是分布式领域的入门必读物。在研究生开始的时候曾经拜读过，当时翻译了全文，当时其实还有很多点没有Get到。同时，论文中个人感觉有的地方它确实并没有讲清楚，使用了非确定的词汇来描述某些情况，对一个喜欢扣细节和有一定强迫症的人来说，无疑也加重了理解难度。例如，GFS可能会在文件中间插入填充数据或者重复记录。
经过了知识的积淀，现在在回过头来理解理解，算是温故求新，另一方面是作为一个存档供以后查阅。
我假定读者都这是对该论文已经通读一篇，所以这里我避开细节，简明扼要的梳理重点。

# 概要


1. GFS是应用层的文件系统，它的下面是正常的linux file system，也就是我们常见的ext4、xfs之类的
2. GFS设计时考虑应对的读场景：频繁大数据量量读，少量随机小数据量读
3. GFS设计时考虑应对的写场景：大量追加，少量修改
4. GFS的API和使用GFS的应用程序协同设计（co-design）增加了系统的灵活性
5. GFS默认组件和网络是不可靠的

显然，GFS不是通用型文件系统，选用HDFS（GFS的开源实现）时需考虑清楚是否符合自己的场景需求。

## GFS提供的功能（interface）

新建文件
打开文件
关闭文件
删除文件
读文件
写文件
基于COW（copy on write）的快照（snapshot）
原子性的追加写记录

## GFS的架构

单Master多Chunk Server结构。

>Master：
* 管理元数据信息
    * 文件和Chunk的命名空间
    * 文件->Chunk的对应关系
    * 每个Chunk->Chunk Server的映射
* 访问控制信息
* 系统管理
    * Chunk租约的管理
    * Orphaned Chunk回收
    * Chunk在Chunk服务器之间的迁移
    * 与所有Chunk Server保持心跳，收集其状态信息
    * 负载均衡
 
备注：
Chunk到Chunk Server的映射是Chunk Server上线主动报告的，Master不会持久化它。

>Chunk Server：
负责客户端真实的读写操作

### 写流程

![gfs_write](/images/gfs_write.jpg)

备注：
写入有两种，一种是客户端给定offset的写入（其实就是修改）；一种是原子性的追加。
多客户端并发写同一个chunk的时候，它可能会写串，写串了之后会走错误流程，进行重试或者恢复。
第二种写入是保证正确的。
写入是强一致性，就是主副chunk都写好了之后，才会返回客户端成功。

|-----|写|记录追加|
|-----|:-----:|------|
|串行成功|已定义的|已定义，部分不一致|
|并行成功|一致但是未定义的|已定义，部分不一致|
|失败|不一致|不一致|

明明在论文中说了已定义了，肯定就是一致的，这里又来个部分不一致，好懵。
论文中还说了：记录追加操作至少可以把数据原子性的追加到文件中一次。也就说原子性的追加写可能会写多次。我的天呐，能不能严谨点，什么鬼啊。最后又说，经过一系列的操作最终region都会是已定义的。

懵。

### 读流程

![gfs_read](/images/gfs_architecture.jpg)

备注：
由于写的时候是强一致性写，所以读的时候主副本都可以读。GFS会保证region最终都是已定义的，它也能区分已定义和未定义。相对而言，读其实没有写那么重要。

## GFS的可靠性

>可靠性：
* 每个Chunk有三份
* 写入时三个Chunk都成功时才会返回成功
* Master有多台热备机，还有“影子”机器（当Master不可用时“影子”负责提供只读服务直到新Master ready）
* 与传统数据库一样，Master通过checkpoint+操作日志的方式增快恢复速度
